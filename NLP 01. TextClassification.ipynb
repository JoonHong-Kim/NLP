{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP 01.Text Classification\n",
    "\n",
    "Category: DeepLearning\n",
    "Tag: DeepLearning, MachineLearning, NLP, 기계학습, 머신러닝\n",
    "Visibility: Public\n",
    "\n",
    "# Text Classification\n",
    "\n",
    "- Input : natural language sentence/paragraph\n",
    "- Output: category → text가 어디에 속해있는가?\n",
    "\n",
    "ex) spam, gmail categorization, election\n",
    "\n",
    " \n",
    "\n",
    "하지만 언어 자체는 arbitrary 함!\n",
    "\n",
    "→ 어떻게 하지 그럼?\n",
    "\n",
    "## Token\n",
    "\n",
    "token은 그냥 아무렇게나 sentence를 쪼개놓은것\n",
    "\n",
    "![NLP%2001%20Text%20Classification%205b9793813b8c4ca09e9085f2aa5177e2/Untitled.png](NLP%2001%20Text%20Classification%205b9793813b8c4ca09e9085f2aa5177e2/Untitled.png)\n",
    "\n",
    "단어들을 찾아서 indexing 해줌\n",
    "\n",
    "→ integer index로 바뀜\n",
    "\n",
    "## Table Lookup\n",
    "\n",
    "하지만 이것도 arbitrary 하다.\n",
    "\n",
    "우리는 neural net 이 의미를 capture할 수 있도록 해야함\n",
    "\n",
    "→ 각 토큰마다 continuous 한 vector를 줌\n",
    "\n",
    "![NLP%2001%20Text%20Classification%205b9793813b8c4ca09e9085f2aa5177e2/Untitled%201.png](NLP%2001%20Text%20Classification%205b9793813b8c4ca09e9085f2aa5177e2/Untitled%201.png)\n",
    "\n",
    "one hot vector와 weight matrix를 곱해줌 \n",
    "\n",
    "이를 Table Lookup이라고 하고 Embedding이라고도 함\n",
    "\n",
    "이건 token에 대한 의미를 찾는다 보면 됨\n",
    "\n",
    "이 첫번째 노드를 지나면 sentence가 sequence of vector가 된다.\n",
    "\n",
    "이를 이용해 계산 후 최종 벡터를 softmax를 사용해 distribution을 구해 category를 정해줌.\n",
    "\n",
    "![NLP%2001%20Text%20Classification%205b9793813b8c4ca09e9085f2aa5177e2/Untitled%202.png](NLP%2001%20Text%20Classification%205b9793813b8c4ca09e9085f2aa5177e2/Untitled%202.png)\n",
    "\n",
    "하지만 sentence의 경우 fixed-size가 아닌데 어떻게 하나?\n",
    "\n",
    "# CBoW\n",
    "\n",
    "Continuous bag-of-words\n",
    "\n",
    "**token의 순서는 무시하겠다!**\n",
    "\n",
    "token vector의 평균을 구함.\n",
    "\n",
    "### Generalizable to bag of N tokens\n",
    "\n",
    "따로 보면 의미가 덜한것들 여러개 묶어서 가능\n",
    "\n",
    "ex) New York University\n",
    "\n",
    "이러한 CBow 방법은 매우 잘 작동함.\n",
    "\n",
    "FastText tool 써서 하면 편함\n",
    "\n",
    "### 과정\n",
    "\n",
    "![NLP%2001%20Text%20Classification%205b9793813b8c4ca09e9085f2aa5177e2/Untitled%203.png](NLP%2001%20Text%20Classification%205b9793813b8c4ca09e9085f2aa5177e2/Untitled%203.png)\n",
    "\n",
    "→ automatic하게 backpropagation 하고 SGD를 이용하여 학습.\n",
    "\n",
    "하지만 이건 단어 순서를 보지않고 하기 때문에 정확도는 높지않음\n",
    "\n",
    "# RN\n",
    "\n",
    "Skip Bigrams\n",
    "\n",
    "문장이 주어졌을 때 모든 token pair를 생각해봄\n",
    "\n",
    "각 페어의 NN를 만들어서 모든 페어의 representation을 찾아줌\n",
    "\n",
    "![NLP%2001%20Text%20Classification%205b9793813b8c4ca09e9085f2aa5177e2/Untitled%204.png](NLP%2001%20Text%20Classification%205b9793813b8c4ca09e9085f2aa5177e2/Untitled%204.png)\n",
    "\n",
    "generalized to triplet 가능하나 computational efficient 안좋음\n",
    "\n",
    "![NLP%2001%20Text%20Classification%205b9793813b8c4ca09e9085f2aa5177e2/Untitled%205.png](NLP%2001%20Text%20Classification%205b9793813b8c4ca09e9085f2aa5177e2/Untitled%205.png)\n",
    "\n",
    "pair-wise 이기 때문에 CBoW보다는 괜찮지만 왜 모든 단어를..?\n",
    "\n",
    " \n",
    "\n",
    "# CNN\n",
    "\n",
    "Convolutional Networks\n",
    "\n",
    "text에서는 1-D convolutional layer\n",
    "\n",
    "![NLP%2001%20Text%20Classification%205b9793813b8c4ca09e9085f2aa5177e2/Untitled%206.png](NLP%2001%20Text%20Classification%205b9793813b8c4ca09e9085f2aa5177e2/Untitled%206.png)\n",
    "\n",
    "계층적으로 가까운거 끼리 묶어서 RN 해주는 느낌\n",
    "\n",
    "CNN은 많이 쓰이기 때문에 operation node에 넣을 것들이 매우 많음.\n",
    "\n",
    "# Self-Attention\n",
    "\n",
    "CNN 단점 → 먼 거리에 중요한 dependency가 있다면 layer를 많이 쌓아야함.\n",
    "\n",
    "RN과 반대인데 이 두가지를 합칠수 없을까..?\n",
    "\n",
    "→ self-attention\n",
    "\n",
    "![NLP%2001%20Text%20Classification%205b9793813b8c4ca09e9085f2aa5177e2/Untitled%207.png](NLP%2001%20Text%20Classification%205b9793813b8c4ca09e9085f2aa5177e2/Untitled%207.png)\n",
    "\n",
    "weight를 0과 1이 아닌 다르게 그리고 자동으로 계산하게끔 해보자.\n",
    "\n",
    "![NLP%2001%20Text%20Classification%205b9793813b8c4ca09e9085f2aa5177e2/Untitled%208.png](NLP%2001%20Text%20Classification%205b9793813b8c4ca09e9085f2aa5177e2/Untitled%208.png)\n",
    "\n",
    "이 알파 function을 이용해서 0과 1사이로 weight sum 해줌.\n",
    "\n",
    "### 장점\n",
    "\n",
    "![NLP%2001%20Text%20Classification%205b9793813b8c4ca09e9085f2aa5177e2/Untitled%209.png](NLP%2001%20Text%20Classification%205b9793813b8c4ca09e9085f2aa5177e2/Untitled%209.png)\n",
    "\n",
    "모든 pair에 대해 알파 값을 구함 → 중요한가 아닌가?\n",
    "\n",
    "이를 RN과 곱해 불필요한 pair는 지우고 해줌.\n",
    "\n",
    "CNN과 RN의 장점 모두 가짐.\n",
    "\n",
    "### 단점\n",
    "\n",
    "![NLP%2001%20Text%20Classification%205b9793813b8c4ca09e9085f2aa5177e2/Untitled%2010.png](NLP%2001%20Text%20Classification%205b9793813b8c4ca09e9085f2aa5177e2/Untitled%2010.png)\n",
    "\n",
    "computational complexity가 너무 높음.\n",
    "\n",
    "counting같은 것들은 힘듦\n",
    "\n",
    "# RNN\n",
    "\n",
    "Online으로 계속 가능하게끔\n",
    "\n",
    "![NLP%2001%20Text%20Classification%205b9793813b8c4ca09e9085f2aa5177e2/Untitled%2011.png](NLP%2001%20Text%20Classification%205b9793813b8c4ca09e9085f2aa5177e2/Untitled%2011.png)\n",
    "\n",
    "구조 자체가 modern하지는 않음.\n",
    "\n",
    "training 시 시간이 걸림\n",
    "\n",
    "LSTM이나 GRU 쓰자.\n",
    "\n",
    "# Summary\n",
    "\n",
    "위 방법들은 다 exclusive 한것들이 아니라 잘 섞어쓰면 더 좋은 결과를 낸다.\n",
    "\n",
    "![NLP%2001%20Text%20Classification%205b9793813b8c4ca09e9085f2aa5177e2/Untitled%2012.png](NLP%2001%20Text%20Classification%205b9793813b8c4ca09e9085f2aa5177e2/Untitled%2012.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
